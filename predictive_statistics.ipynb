{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Chnage in Perspective for Predictive Modeling \n",
    "### Goodness of fit, train test split and bias variance tradeoff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Modeling vs Inferential Modeling\n",
    "\n",
    "Previously we have done statistical modeling with the intent to do inference on the resulting fit model. With predictive modeling the resulting structure of our model is simply a means to a predictive ends.\n",
    "\n",
    "#### How did we decide if a model was good when we were attempting to do inferential statistics?\n",
    "\n",
    "We made sure the model met all the assumptions of our linear regression then used the coefficients of our model to make claims about the world.\n",
    "\n",
    "We also used $R^2$ to understand how much of the variance of our model was explained by our features. Even if we had a low $R^2$ we could still make claims about the world **if** our model assumptions held."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a toy dataset to look at model assumptions for linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 10\n",
    "\n",
    "noise = np.random.normal(0, 8, size=SIZE)\n",
    "x = np.random.uniform(0, 10, size=SIZE)\n",
    "\n",
    "intercept = 2\n",
    "\n",
    "# y = m + b + some_noise\n",
    "y = intercept + 4*x + noise\n",
    "\n",
    "# reshape is necessary because there is only one feature\n",
    "lr = LinearRegression()\n",
    "lr.fit(x.reshape(-1, 1), y)\n",
    "\n",
    "print(f'Intercept: {lr.intercept_:.2f} Coefficeint: {lr.coef_[0]:.2f} R2: {lr.score(x.reshape(-1, 1), y):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Python Tip:** using the variable:.2f syntax inside of an f-string allows you to format the output of the variable. Lets look at what happens when I remove the extra formatting syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Intercept: {lr.intercept_} Coefficeint: {lr.coef_[0]} R2: {lr.score(x.reshape(-1, 1), y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(7, 7))\n",
    "\n",
    "ax[0, 0].scatter(x, y, alpha=.5)\n",
    "\n",
    "pred = lr.predict(x.reshape(-1, 1))\n",
    "ax[0, 0].plot(x, pred, color='orange')\n",
    "ax[0, 0].set_title(f'Intercept: {lr.intercept_:.2f} Coeff: {lr.coef_[0]:.2f}')\n",
    "\n",
    "residuals =  y - lr.predict(x.reshape(-1, 1))\n",
    "ax[0, 1].scatter(x, residuals, alpha=.5)\n",
    "ax[0, 1].axhline(0, color='orange')\n",
    "ax[0, 1].set_title('Residuals')\n",
    "\n",
    "ax[1, 0].hist(residuals, bins=20)\n",
    "ax[1, 0].axvline(residuals.mean(), linestyle='--', color='orange')\n",
    "ax[1, 0].set_title('Histogram of Residuals')\n",
    "\n",
    "ax[1, 1].set_title('QQ Plot')\n",
    "sm.qqplot(residuals, ax=ax[1, 1], fit=True, color='orange', line='q')\n",
    "\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are some ways we can measure the goodness of a model if we only care about predictions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(14, 5))\n",
    "\n",
    "# linear model on a linear relationship\n",
    "axes[0].scatter(x, y)\n",
    "pred = lr.predict(x.reshape(-1, 1))\n",
    "axes[0].plot(x, pred, color='orange')\n",
    "axes[0].set_title(f'Model 1 Intercept: {lr.intercept_:.2f} Slope: {lr.coef_[0]:.2f}')\n",
    "\n",
    "# linear model on an exponential relationship\n",
    "\n",
    "SIZE = 10\n",
    "\n",
    "noise = np.random.normal(0, 8, size=SIZE)\n",
    "x2 = np.random.uniform(0, 10, size=SIZE)\n",
    "\n",
    "intercept2 = 10\n",
    "\n",
    "# y = m + b + some_noise\n",
    "y2 = intercept2 + 1.3*x2**2 + noise\n",
    "\n",
    "lr2 = LinearRegression()\n",
    "lr2.fit(x2.reshape(-1,1), y2)\n",
    "\n",
    "axes[1].scatter(x2, y2)\n",
    "pred2 = lr2.predict(x2.reshape(-1, 1))\n",
    "\n",
    "axes[1].plot(x2, pred2, color='orange')\n",
    "axes[1].set_title(f'Model 2 Intercept: {lr2.intercept_:.2f} Slope: {lr2.coef_[0]:.2f}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression model metrics: Mean Squared Error and Mean Absolute Error\n",
    "\n",
    "**CONCEPT CHECK: Why do we need to square or take the absolute value of the residuals?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$MSE=\\frac{1}{n}\\sum_{i=1}^n (y_{i} - \\hat{y_{i}})^2$$\n",
    "\n",
    "$$MAE=\\frac{1}{n}\\sum_{i=1}^n |y_{i} - \\hat{y_{i}}|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these metrics compare the true values of each observation with the predicted values. The difference between the two metrics comes from how negative values are delt with, absolute value or squaring. MSE punishes larger deviations from the true value proportionally more than with MAE. Both metrics are commonly used and mostly depends on the particular situation you are modeling. Lets take a look at the MSE and MAE for model 1 and model 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Sklearn.metrics Docs](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics) includes many common metrics for regression, clustering and classification problems. We will just be using MAE and MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model 1')\n",
    "print(f'MAE: {mean_absolute_error(pred, y):.2f} MSE: {mean_squared_error(pred, y):.2f}')\n",
    "print('------------------------')\n",
    "print('Model 2')\n",
    "print(f'MAE: {mean_absolute_error(pred2, y2):.2f} MSE: {mean_squared_error(pred2, y2):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we notice here is that MSE always appears to be a lot higher than MAE. The reason for this is that MSE is actually in different units. Since we squared our values MSE is in units squared. For example, if we calculated MSE for our housing prices our value would be in squared dollars. It is fairly common to use root mean squared error (RMSE) instead of just MSE to see what the average error of our model is in the same units as we started.\n",
    "\n",
    "This can calculated by taking the square root of mean_squared_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model 1')\n",
    "print(f'MAE: {mean_absolute_error(pred, y):.2f} RMSE: {np.sqrt(mean_squared_error(pred, y)):.2f}')\n",
    "print('------------------------')\n",
    "print('Model 2')\n",
    "print(f'MAE: {mean_absolute_error(pred2, y2):.2f} RMSE: {np.sqrt(mean_squared_error(pred2, y2)):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective function vs an Evaluation metric\n",
    "\n",
    "The objective function is the function which the model is directly optimizing for which guides how the final fit model will look.\n",
    "\n",
    "An evaluation function is the function which you use after the fitting to decide how good the model is.\n",
    "\n",
    "These can be the same thing for example MSE is used as the objective function in linear regression and can also be used to evaluate the model afterwards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding complexity to our models: Polynomial Regression\n",
    "\n",
    "Looking at model 2 we can see that there is a non-linear relationship between x and y. It looks like an exponential relationship so lets try to fit a polynomial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Instantiate our polynomial features object with degree of 2\n",
    "pf = PolynomialFeatures(degree=2)\n",
    "\n",
    "# Fit and transform our data\n",
    "poly_data = pf.fit_transform(x2.reshape(-1, 1))\n",
    "\n",
    "# Instantiate linear regression model and fit it to our transformed features\n",
    "lr_poly = LinearRegression()\n",
    "lr_poly.fit(poly_data, y2)\n",
    "\n",
    "x_ = np.linspace(0, 10, num=100)\n",
    "poly_x_ = pf.transform(x_.reshape(-1, 1))\n",
    "poly_ = lr_poly.predict(poly_x_)\n",
    "\n",
    "# Predict on data\n",
    "poly_preds = lr_poly.predict(poly_data)\n",
    "\n",
    "print('Polynomial Model with degree 2')\n",
    "print(f'MAE: {mean_absolute_error(poly_preds, y2):.2f} RMSE: {np.sqrt(mean_squared_error(poly_preds, y2)):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "ax.scatter(x2, y2, alpha=.5)\n",
    "ax.plot(x_, poly_, color='orange');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complexity?\n",
    "\n",
    "This fit looks good but we are still getting some error. Maybe if we continue to add higher order polynomials we will continue to reduce our error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [1, 2, 3, 4]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "ax.set_ylim(0, 150)\n",
    "\n",
    "for degree in degrees:\n",
    "    # Instantiate our polynomial features object with degree of 2\n",
    "    pf = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Fit and transform our data\n",
    "    poly_data = pf.fit_transform(x2.reshape(-1, 1))\n",
    "\n",
    "    # Instantiate linear regression model and fit it to our transformed features\n",
    "    lr_poly = LinearRegression()\n",
    "    lr_poly.fit(poly_data, y2)\n",
    "\n",
    "    x_ = np.linspace(0, 10, num=100)\n",
    "    poly_x_ = pf.transform(x_.reshape(-1, 1))\n",
    "    poly_ = lr_poly.predict(poly_x_)\n",
    "\n",
    "    # Predict on data\n",
    "    poly_preds = lr_poly.predict(poly_data)\n",
    "    \n",
    "    # plot\n",
    "    ax.plot(x_, poly_, label=f'degree {degree}')\n",
    "    \n",
    "    \n",
    "\n",
    "    print(f'Polynomial Model with degree {degree}')\n",
    "    print(f'MAE: {mean_absolute_error(poly_preds, y2):.2f} RMSE: {np.sqrt(mean_squared_error(poly_preds, y2)):.2f} coefs: {lr_poly.coef_[1:]} {lr_poly.intercept_:.2f}')\n",
    "    print('----------')\n",
    "ax.scatter(x2, y2)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase the degree of our polynomial we continue to reduce our error, but the models that are being produced look wildly different. Which model would you say is the best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error due to Bias and Error Due to Variance\n",
    "\n",
    "Error due to Bias: The error due to bias is taken as the difference between the expected (or average) prediction of our model and the correct value which we are trying to predict. Of course you only have one model so talking about expected or average prediction values might seem a little strange. However, imagine you could repeat the whole model building process more than once: each time you gather new data and run a new analysis creating a new model. Due to randomness in the underlying data sets, the resulting models will have a range of predictions. Bias measures how far off in general these models' predictions are from the correct value.\n",
    "\n",
    "\n",
    "Error due to Variance: The error due to variance is taken as the variability of a model prediction for a given data point. Again, imagine you can repeat the entire model building process multiple times. The variance is how much the predictions for a given point vary between different realizations of the model.\n",
    "\n",
    "source: http://scott.fortmann-roe.com/docs/BiasVariance.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting and Underfitting: The train test split\n",
    "\n",
    "The error that we have been calculating is known as the in sample error. This is the error from the predictions made on the data the model was trained on. While this number does give us some information the more important information comes from the out of sample error.\n",
    "\n",
    "Out of sample error measures how badly a model will do on data that it has not been trained on. We can calculated this in many ways but the most simple method is to split our data into two parts, training and testing. We will fit our model on the training data and calculate our error on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "data = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
    "\n",
    "df['prices'] = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df[['AGE', 'LSTAT', 'RM']], df[['prices']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error = []\n",
    "test_error = []\n",
    "\n",
    "degrees = range(8)\n",
    "\n",
    "for degree in degrees:\n",
    "    pf = PolynomialFeatures(degree=degree)\n",
    "    \n",
    "    poly_train = pf.fit_transform(X_train)\n",
    "    poly_test = pf.transform(X_test)\n",
    "\n",
    "    # Instantiate linear regression model and fit it to our transformed features\n",
    "    lr_poly = LinearRegression()\n",
    "    lr_poly.fit(poly_train, y_train)\n",
    "    \n",
    "    train_preds = lr_poly.predict(pf.transform(X_train))\n",
    "    test_preds = lr_poly.predict(pf.transform(X_test))\n",
    "    \n",
    "    train_RMSE = np.sqrt(mean_squared_error(train_preds, y_train))\n",
    "    test_RMSE = np.sqrt(mean_squared_error(test_preds, y_test))\n",
    "    \n",
    "    train_error.append(train_RMSE)\n",
    "    test_error.append(test_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "\n",
    "ax.set_ylim(0, 100)\n",
    "ax.plot(degrees, train_error, label='training error')\n",
    "ax.plot(degrees, test_error, label='testing error')\n",
    "ax.axvline(degrees[np.argmin(test_error)], linestyle='--', color='black')\n",
    "ax.set_xlabel('Degree')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalizing from our data\n",
    "\n",
    "The training error continues to get smaller or remains low while as we increase the degree of the polynomials used the testing error explodes.\n",
    "\n",
    "We can see that the lowest RMSE model that we looked at is not the most complex. When the model complexity becomes to high it will begin to fit to the noise in the data rather than the underlying signal and failing to be predictive on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pitfalls of the train test split: Is this an accurate representation of how well our model will do in the future?\n",
    "\n",
    "Maybe not. \n",
    "- We could be over fitting to our testing set\n",
    "- The testing set may not be representative of future data\n",
    "- Data leakage: we gave the model information about the future that it won't have with new data\n",
    "\n",
    "**Every time we make modeling decisions based on our hold out set we increase the risk of overfitting to our data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
